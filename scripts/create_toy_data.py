#!/usr/bin/env python3
"""
ì›ë³¸ Parquet ë°ì´í„°ì—ì„œ toy ë°ì´í„°ì…‹ ìƒì„±
ë¹ ë¥¸ ì‹¤í—˜ì„ ìœ„í•´ 100,000ê°œ ìƒ˜í”Œë§Œ ì¶”ì¶œí•˜ì—¬ CSVë¡œ ì €ì¥
"""

import pandas as pd
import pyarrow.parquet as pq
from pathlib import Path
import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def create_toy_dataset(
    input_path: str,
    output_path: str,
    n_samples: int = 100000,
    random_state: int = 42,
    method: str = 'sequential'
):
    """
    Parquet íŒŒì¼ì—ì„œ ìƒ˜í”Œë§í•˜ì—¬ toy CSV ìƒì„± (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
    
    Args:
        input_path: ì…ë ¥ parquet íŒŒì¼ ê²½ë¡œ
        output_path: ì¶œë ¥ CSV íŒŒì¼ ê²½ë¡œ
        n_samples: ì¶”ì¶œí•  ìƒ˜í”Œ ìˆ˜
        random_state: ëœë¤ ì‹œë“œ
        method: 'sequential' (ì•ì—ì„œë¶€í„°) ë˜ëŠ” 'reservoir' (ëœë¤ ìƒ˜í”Œë§)
    """
    logging.info(f"Reading parquet file: {input_path}")
    
    # Parquet íŒŒì¼ ë©”íƒ€ë°ì´í„° í™•ì¸
    pf = pq.ParquetFile(input_path)
    total_rows = pf.metadata.num_rows
    logging.info(f"Total rows in original file: {total_rows:,}")
    
    if n_samples >= total_rows:
        logging.warning(f"Requested samples ({n_samples}) >= total rows ({total_rows}). Using all data.")
        n_samples = total_rows
    
    if method == 'sequential':
        # ë°©ë²• 1: ì•ì—ì„œë¶€í„° n_samplesê°œë§Œ ì½ê¸° (ê°€ì¥ ë¹ ë¦„)
        logging.info(f"Using sequential method: reading first {n_samples:,} rows")
        
        # iter_batchesë¡œ í•„ìš”í•œ ë§Œí¼ë§Œ ì½ê¸°
        rows_collected = 0
        dfs = []
        
        for batch in pf.iter_batches(batch_size=50000):
            batch_df = batch.to_pandas()
            remaining = n_samples - rows_collected
            
            if remaining <= len(batch_df):
                # í•„ìš”í•œ ë§Œí¼ë§Œ ê°€ì ¸ì˜¤ê³  ì¢…ë£Œ
                dfs.append(batch_df.head(remaining))
                break
            else:
                dfs.append(batch_df)
                rows_collected += len(batch_df)
                logging.info(f"Collected {rows_collected:,} / {n_samples:,} rows")
        
        df_toy = pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()
        
    elif method == 'reservoir':
        # ë°©ë²• 2: Reservoir Sampling (ì§„ì§œ ëœë¤ ìƒ˜í”Œë§, ì¡°ê¸ˆ ëŠë¦¼)
        logging.info(f"Using reservoir sampling method for true random sampling")
        import numpy as np
        np.random.seed(random_state)
        
        # ì²­í¬ë¡œ ì½ìœ¼ë©´ì„œ Reservoir Sampling
        chunk_size = 100000
        reservoir = None
        rows_seen = 0
        
        for batch in pf.iter_batches(batch_size=chunk_size):
            chunk_df = batch.to_pandas()
            chunk_size_actual = len(chunk_df)
            
            if reservoir is None:
                # ì²« ì²­í¬: ì²˜ìŒ n_samplesê°œë¥¼ reservoirë¡œ
                reservoir = chunk_df.head(n_samples).copy()
                rows_seen = chunk_size_actual
            else:
                # ì´í›„ ì²­í¬: reservoir sampling ì•Œê³ ë¦¬ì¦˜
                for idx in range(chunk_size_actual):
                    rows_seen += 1
                    # í™•ë¥ ì ìœ¼ë¡œ ê¸°ì¡´ ìƒ˜í”Œì„ êµì²´
                    j = np.random.randint(0, rows_seen)
                    if j < n_samples:
                        reservoir.iloc[j] = chunk_df.iloc[idx]
            
            if rows_seen % 500000 == 0:
                logging.info(f"Processed {rows_seen:,} rows...")
        
        df_toy = reservoir.reset_index(drop=True)
    
    else:
        raise ValueError(f"Unknown method: {method}")
    
    logging.info(f"Loaded dataframe shape: {df_toy.shape}")
    logging.info(f"Columns: {list(df_toy.columns)}")
    logging.info(f"Final sample size: {len(df_toy):,} rows")
    
    # ë ˆì´ë¸” ë¶„í¬ í™•ì¸
    if 'clicked' in df_toy.columns:
        click_rate = df_toy['clicked'].mean()
        logging.info(f"Click rate: {click_rate:.4f}")
        logging.info(f"Clicked: {df_toy['clicked'].sum():,}, Not clicked: {(~df_toy['clicked'].astype(bool)).sum():,}")
    
    # CSVë¡œ ì €ì¥
    output_path = Path(output_path)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    df_toy.to_csv(output_path, index=False)
    logging.info(f"âœ… Saved toy dataset to: {output_path}")
    logging.info(f"File size: {output_path.stat().st_size / 1024 / 1024:.2f} MB")
    
    return df_toy


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description='Create toy dataset from parquet')
    parser.add_argument('--input', type=str, default='data/raw/train.parquet',
                       help='Input parquet file path')
    parser.add_argument('--output', type=str, default='data/toy/train_toy.csv',
                       help='Output CSV file path')
    parser.add_argument('--n_samples', type=int, default=100000,
                       help='Number of samples to extract')
    parser.add_argument('--seed', type=int, default=42,
                       help='Random seed')
    parser.add_argument('--method', type=str, default='sequential',
                       choices=['sequential', 'reservoir'],
                       help='Sampling method: sequential (fast) or reservoir (true random)')
    
    args = parser.parse_args()
    
    create_toy_dataset(
        input_path=args.input,
        output_path=args.output,
        n_samples=args.n_samples,
        random_state=args.seed,
        method=args.method
    )
    
    logging.info("ğŸ‰ Toy dataset creation completed!")


if __name__ == "__main__":
    main()

