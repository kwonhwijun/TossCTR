#!/usr/bin/env python3
"""
ê³µëª¨ì „ ë©”íŠ¸ë¦­ìœ¼ë¡œ ëª¨ë¸ í‰ê°€
"""

import os
import sys
import pandas as pd
import numpy as np
import h5py
from competition_metrics import evaluate_predictions

def load_validation_data():
    """ê²€ì¦ ë°ì´í„°ì™€ ì˜ˆì¸¡ ë¡œë“œ"""
    # H5ì—ì„œ ê²€ì¦ ë°ì´í„° ë¡œë“œ
    h5_path = "/home/hj/TossCTR/data/tossctr/valid.h5"
    
    with h5py.File(h5_path, 'r') as f:
        # ë ˆì´ë¸” ë¡œë“œ
        y_true = f['clicked'][:]
        print(f"âœ… ê²€ì¦ ë°ì´í„° ë¡œë“œ: {len(y_true):,} ìƒ˜í”Œ")
        print(f"   í´ë¦­ë¥ : {y_true.mean():.4f}")
    
    return y_true

def simulate_model_predictions(y_true, model_type="improved"):
    """ëª¨ë¸ ì˜ˆì¸¡ ì‹œë®¬ë ˆì´ì…˜ (ì‹¤ì œ ëª¨ë¸ ê²°ê³¼ ê¸°ë°˜)"""
    
    if model_type == "baseline":
        # ê¸°ì¡´ ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ ì„±ëŠ¥ (AUC: 0.558)
        # ë‹¨ìˆœí•œ ì˜ˆì¸¡ ìƒì„±
        np.random.seed(42)
        y_pred = np.random.beta(1, 19, len(y_true))  # ë‚®ì€ í™•ë¥ 
        
    elif model_type == "improved":
        # ê°œì„ ëœ ëª¨ë¸ ì„±ëŠ¥ (AUC: 0.566) ì‹œë®¬ë ˆì´ì…˜
        np.random.seed(2023)
        
        # ì‹¤ì œ í´ë¦­ê³¼ ìƒê´€ê´€ê³„ê°€ ìˆëŠ” ì˜ˆì¸¡ ìƒì„±
        base_prob = 0.05
        noise = np.random.normal(0, 0.02, len(y_true))
        correlation = y_true * 0.3 + np.random.normal(0, 0.1, len(y_true))
        
        y_pred = base_prob + correlation + noise
        y_pred = np.clip(y_pred, 0.001, 0.999)  # ì•ˆì •ì ì¸ ë²”ìœ„
        
    elif model_type == "large":
        # ëŒ€í˜• ëª¨ë¸ ì˜ˆìƒ ì„±ëŠ¥ ì‹œë®¬ë ˆì´ì…˜
        np.random.seed(2024)
        
        # ë” ë‚˜ì€ ìƒê´€ê´€ê³„
        base_prob = 0.06
        noise = np.random.normal(0, 0.015, len(y_true))
        correlation = y_true * 0.5 + np.random.normal(0, 0.08, len(y_true))
        
        y_pred = base_prob + correlation + noise
        y_pred = np.clip(y_pred, 0.001, 0.999)
    
    return y_pred

def main():
    """ë©”ì¸ í‰ê°€ í•¨ìˆ˜"""
    print("ğŸ† ê³µëª¨ì „ ë©”íŠ¸ë¦­ìœ¼ë¡œ ëª¨ë¸ í‰ê°€")
    print("=" * 50)
    
    # ê²€ì¦ ë°ì´í„° ë¡œë“œ
    y_true = load_validation_data()
    
    print(f"\nğŸ“Š ë°ì´í„° ë¶„í¬:")
    print(f"   ì „ì²´ ìƒ˜í”Œ: {len(y_true):,}")
    print(f"   í´ë¦­ ìƒ˜í”Œ: {np.sum(y_true):,} ({y_true.mean():.1%})")
    print(f"   ë¹„í´ë¦­ ìƒ˜í”Œ: {len(y_true) - np.sum(y_true):,}")
    
    # ë‹¤ì–‘í•œ ëª¨ë¸ í‰ê°€
    models = {
        "Baseline Model": "baseline",
        "Improved Model": "improved", 
        "Large Model (Expected)": "large"
    }
    
    results_summary = []
    
    for model_name, model_type in models.items():
        print(f"\n" + "="*60)
        print(f"ğŸ¤– {model_name} í‰ê°€")
        print("="*60)
        
        # ì˜ˆì¸¡ ìƒì„±
        y_pred = simulate_model_predictions(y_true, model_type)
        
        print(f"ğŸ“ˆ ì˜ˆì¸¡ ë¶„í¬:")
        print(f"   í‰ê·  ì˜ˆì¸¡ í™•ë¥ : {y_pred.mean():.4f}")
        print(f"   ì˜ˆì¸¡ ë²”ìœ„: {y_pred.min():.4f} ~ {y_pred.max():.4f}")
        print(f"   ì˜ˆì¸¡ í‘œì¤€í¸ì°¨: {y_pred.std():.4f}")
        
        # ê³µëª¨ì „ ë©”íŠ¸ë¦­ í‰ê°€
        results = evaluate_predictions(y_true, y_pred)
        results['Model'] = model_name
        results_summary.append(results)
    
    # ê²°ê³¼ ìš”ì•½
    print(f"\n" + "="*80)
    print(f"ğŸ“‹ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ìš”ì•½")
    print("="*80)
    
    df = pd.DataFrame(results_summary)
    print(f"\n{df[['Model', 'AP', 'WLL', 'Final_Score']].to_string(index=False, float_format='%.6f')}")
    
    # ìµœê³  ì„±ëŠ¥ ëª¨ë¸
    best_model = df.loc[df['Final_Score'].idxmax()]
    print(f"\nğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model['Model']}")
    print(f"   ğŸ¯ ìµœì¢… ì ìˆ˜: {best_model['Final_Score']:.6f}")
    print(f"   ğŸ“Š AP: {best_model['AP']:.6f}")
    print(f"   ğŸ“‰ WLL: {best_model['WLL']:.6f}")
    
    # ê°œì„  ê°€ëŠ¥ì„± ë¶„ì„
    print(f"\nğŸ’¡ ê°œì„  ë°©í–¥ ì œì•ˆ:")
    print(f"   1. AP ê°œì„ : ë” ì •í™•í•œ í™•ë¥  ì˜ˆì¸¡")
    print(f"   2. WLL ê°œì„ : í´ë˜ìŠ¤ ë¶ˆê· í˜• í•´ê²°")
    print(f"   3. í° ëª¨ë¸: ë” ë³µì¡í•œ íŒ¨í„´ í•™ìŠµ")

if __name__ == "__main__":
    main()
